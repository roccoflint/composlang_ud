{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legitimize probabilties:\n",
    "\n",
    "The probabilities we're currently using right now (particularly in the zoomed-in 1000x1000 analysis) are a mess: they are normalized within the 1000x1000 window rather than over the entire corpus; I don't know what the marginal distribution really should be; etc. In this notebook I'll write down some conventions to use in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w.r.t. individual lexical probability we have two choices:\n",
    "1. `p(a) = {#(a) / |A|}` ... meaning, we normalize adj. freq by size of the adjective class\n",
    "2. `p(a) = {#(a) / |corpus|}` ... meaning, we normalize by the total size of the corpus (# tokens)\n",
    "3. `p(a) = {#(a) / (|A|+|N|)}` ... meaning, we normaize by the shared total size of Adj and Nouns.\n",
    "\n",
    "with [2] and [3], `p(a)` and `p(n)` would share a common denominator, and be comparable (this is a plus!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w.r.t joint probability, we have co-occurrence data $f_{ij} = \\#(a_i n_j)$.\n",
    "$$p(a_i n_j) = \\frac{ \\#(a_i n_j) }{\n",
    "    \\sum_{j=1}^{|N|} \\sum_{i=1}^{|A|} \\#(a_i n_j)\n",
    "}$$\n",
    "\n",
    "what is this the probability of? this is the probability of encountering Adj-N pair $a_in_j$ \n",
    "among all possible Adj-N pairs.\n",
    "an alternative way to normalize this would have been to consider all two-item pairs of any UPOS, but that's\n",
    "equivalent to the decision to be made between [2] and [3] above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is also true that\n",
    "$$\n",
    "p(a_in_j) \\;\\; \\propto \\;\\; \\#(a_in_j)\n",
    "$$\n",
    "because the denominator is common for each pair. so normalizing within our subset of interest was fine.\n",
    "\n",
    "Let \n",
    "$T = \\sum_{j=1}^{|N|} \\sum_{i=1}^{|A|} \\#(a_i n_j)$,\n",
    "and\n",
    "$T' = \\sum_{j=1}^{1000} \\sum_{i=1}^{1000} \\#(a_i n_j)$\n",
    "\n",
    "then \n",
    "$$\n",
    "p(a_i n_j) =  \\frac{\\#(a_i n_j)}{T'} \\cdot \\frac{T'}{T} = p'(a_in_j) \\frac{T'}{T}\n",
    "$$\n",
    "\n",
    "however: how does this change conditional probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(n_j | a_i) = \\frac{\n",
    "    \\#(a_i n_j)\n",
    "}{\n",
    "    \\sum_{j'=1}^{|N|} \\#(a_in_{j'})\n",
    "}\n",
    "=\n",
    "\\frac{p(a_in_j)}{p(a_i)} = \\frac{p'(a_in_j)}{p(a_i)}\\frac{T'}{T} \\; \\propto \\; \\frac{p'(a_in_j)}{p(a_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we'll load co-occurrence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from composlang.utils import minmax\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import logsumexp\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_freq = pd.read_pickle(\".//adj_freqs.pkl\")\n",
    "noun_freq = pd.read_pickle(\".//noun_freqs.pkl\")\n",
    "pair_freq = pd.read_pickle(\".//pair_freq.pkl\")\n",
    "freqs = dict(adj_freq=adj_freq, noun_freq=noun_freq, pair_freq=pair_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = 34,994,090    N = 104,576,156\n"
     ]
    }
   ],
   "source": [
    "A = sum(adj_freq.values())\n",
    "N = sum(noun_freq.values())\n",
    "print(f\"{A = :,}    {N = :,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AN = 25,178,311\n"
     ]
    }
   ],
   "source": [
    "AN = sum(pair_freq.values())\n",
    "print(f\"{AN = :,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_p = {k: np.log(v / (A)) for k, v in adj_freq.items()}\n",
    "noun_p = {k: np.log(v / (N)) for k, v in noun_freq.items()}\n",
    "pair_p = {k: np.log(v / (AN)) for k, v in pair_freq.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that these are all probability distributions, normalized over sensible normalizing factors, \n",
    "we can verify some of their properties.\n",
    "\n",
    "They should all individually add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0000000000032314, 0.9999999999912396, 1.0000000001624214)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(np.exp, adj_p.values())), sum(map(np.exp, noun_p.values())), sum(\n",
    "    map(np.exp, pair_p.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(adj_p, \"./adj_p.pkl\")\n",
    "pd.to_pickle(noun_p, \"./noun_p.pkl\")\n",
    "pd.to_pickle(pair_p, \"./pair_p.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus-based conditionals \n",
    "Now within the subset $A_{<1000}, N_{<1000}$ we want to approximate (estimating these for the full joint distribution will take too long)\n",
    "First, create a joint distribution matrix to make conditional normalizing operations easier.\n",
    "\n",
    "Le `joint` be our joint distribution for the top 1000 lexical items.\n",
    "`joint[i,j]` = $\\log p_{corpus}(a_i, n_j)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointDist:\n",
    "    def __init__(\n",
    "        self,\n",
    "        adj_weights: typing.Collection[typing.Tuple[str, float]],\n",
    "        noun_weights: typing.Collection[typing.Tuple[str, float]],\n",
    "        pair_weights: typing.Collection[\n",
    "            typing.Tuple[\n",
    "                typing.Tuple[str, str],\n",
    "                float,\n",
    "            ]\n",
    "        ],\n",
    "        m=1_000,\n",
    "    ):\n",
    "        import numpy as np\n",
    "        from scipy.special import logsumexp\n",
    "\n",
    "        self.m = m\n",
    "        self.adj_index: typing.Dict[str, int] = {}\n",
    "        self.noun_index: typing.Dict[str, int] = {}\n",
    "        self.adj_weights = []\n",
    "        self.noun_weights = []\n",
    "\n",
    "        for i, a in enumerate(adj_weights):\n",
    "            if i >= m:\n",
    "                break\n",
    "            if a not in self.adj_index:\n",
    "                self.adj_index[a] = len(self.adj_index)\n",
    "                self.adj_weights.append(adj_weights[a])\n",
    "        for i, n in enumerate(noun_weights):\n",
    "            if i >= m:\n",
    "                break\n",
    "            if n not in self.noun_index:\n",
    "                self.noun_index[n] = len(self.noun_index)\n",
    "                self.noun_weights.append(noun_weights[n])\n",
    "\n",
    "        self.joint = np.zeros((m, m)) - np.inf\n",
    "        for an, p in tqdm(pair_weights.items(), total=len(pair_weights)):\n",
    "            try:\n",
    "                self[an.split()] = p\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except TypeError:\n",
    "                print(an)\n",
    "\n",
    "        # normalize to get a proper joint distribution\n",
    "        self.joint -= logsumexp(self.joint)\n",
    "\n",
    "    def get_index(self, adj, noun) -> typing.Tuple[int, int]:\n",
    "        aix = self.adj_index[adj] if isinstance(adj, str) else adj or ...\n",
    "        nix = self.noun_index[noun] if isinstance(noun, str) else noun or ...\n",
    "        return aix, nix\n",
    "\n",
    "    def __getitem__(self, key) -> float:\n",
    "        if isinstance(key, str):\n",
    "            key = key.split()\n",
    "        ix = self.get_index(*key)\n",
    "        return self.joint[ix]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        ix = self.get_index(*key)\n",
    "        self.joint[ix] = value\n",
    "\n",
    "    def conditionalize(self, axis: int = 0) -> \"JointDist\":\n",
    "        \"\"\"\n",
    "        axis 0 corresponds to marginalizing over adjectives to get p(n|a).\n",
    "            we sum over axis 0 (sum the distribution corresponding to each adjective)\n",
    "            and divide by it, so that row is left with p(n|a)\n",
    "        axis 1 corresponds to marginalizing over nouns to get p(a|n) (by summing over axis 1)\n",
    "            we sum over axis 1 (sum the distribution corresponding to each noun)\n",
    "            and divide by it, so that column is left with p(a|n)\n",
    "        \"\"\"\n",
    "        import copy\n",
    "        from scipy.special import logsumexp\n",
    "\n",
    "        if axis == 0:\n",
    "            new_joint = self.joint - logsumexp(self.joint, axis=1 - axis)[:, None]\n",
    "        elif axis == 1:\n",
    "            new_joint = self.joint - logsumexp(self.joint, axis=1 - axis)\n",
    "        else:\n",
    "            raise ValueError(\"axis must be 0 or 1 for bivariate joint distribution\")\n",
    "        self_copy = copy.deepcopy(self)\n",
    "        self_copy.joint = new_joint\n",
    "        return self_copy\n",
    "\n",
    "    def get_marginal_of_axis(self, axis: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        returns the marginal distribution (one-dimensional) along the specified axis\n",
    "        axis 0 corresponds to marginalizing  to get p(a)\n",
    "        \"\"\"\n",
    "        return self.joint.sum(axis=1 - axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.877082674548296"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsumexp(joint.joint, axis=1)[987]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.877082674548296"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsumexp(joint[\"peculiar\", ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'off-'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def invert_dict(d):\n",
    "    return {v: k for k, v in d.items()}\n",
    "\n",
    "\n",
    "invert_dict(joint.noun_index)[987]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010283708572387695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4423676,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5919b99c6cb04635bebe5005d8fa812f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4423676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other :   --\n",
      "serum 25-hydroxyvitamin bone\n",
      "poor t .\n",
      "quick .   --\n",
      "beautiful .   --\n",
      "white can ,\n"
     ]
    }
   ],
   "source": [
    "joint = JointDist(adj_p, noun_p, pair_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsumexp(joint[:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = joint.conditionalize(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excellent! We have a joint distribution that sums up to 1 (0) now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(joint, \"./joint1000.pkl\")\n",
    "pd.to_pickle(cond, \"./cond_N_A_1000.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "composlang-iD_d0IlX-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
